### Exploring Limits to Prediction in Complex Social Systems ([Link](https://dl.acm.org/doi/pdf/10.1145/2872427.2883001))

Looking at a different direction from papers from last week, this paper studies the problem of "prediction using social data" itself, analysing whether this is indeed a problem that can be solved, and whether it is a problem that can be solved with a good enough accuracy. The paper lists down the various problems that come with prediction studies - lack of a solid evaluation framework being one. This paper formalizes one such framework to evaluate _ex-ante_ prediction success. Secondly, it also runs a case study of its own, to study the factors influencing prediction. Lastly, it runs a simulation study on a Twitter-like network to study the amount of information needed for a good *ex-ante* prediction. Given the flood of prediction studies that have taken off in the past few years with the loads of social media data available at the drop of a hat, this study couldn't have come at a better time - questioning the effectiveness of these studies and asking important questions to improve prediction study designs of the future.  
The paper did a great job at study design - right from asking the right questions, they also went on to demonstrate how their framework can be used by researchers to evaluate their problem statements, and went even a step further to simulate network effects to prove their problem statement that social media data alone cannot provide enough information to reliably predict a variable, there's an underlying generative process that we still haven't explained. The cascade simulation of study was also very well designed, to include uncertainities just like the real world in terms of ex-ante knowledge available for prediction, making it very realisitic.  
In the end, the paper suggests that there are limitations to how well we can predict real-world events based on data alone, and they point out the various factors that can impact these predictions. Having said that, there are some gaps that future papers can pick up, one being that this study focuses on retweet cascade prediction as the problem statement, in which, there's a lot more unknowns and variability compared to some other real-world problems like box office collections or flu prediction. Thus, design of problem statement itself can be something that affects the bounds of prediction that the authors mention.  

---


### The Parable of Google Flu: Traps in Big Data Analysis ([Link](https://www.science.org/doi/epdf/10.1126/science.1248506))

This article also studies the same problem as the previous one did - understanding whether a prediction model is good enough, and what might be affecting the performance of such models for a given problem statement. It looks at the Google Flu Trends system, and comparing it with the more traditional ways of prediction, looks at generalizing the gaps in prediction systems of the day.  
This is a very interesting study at the intersection of academia and business, studying how mixing the two together without coordination can lead to disastrous results in one or both domains. It touches upon some techniques that we have seen in previous papers like snowball sampling, and is a good article to understand the pros and cons of such methods. Finally, the article also mentions the oft-repeated parable that when features turn into metrics, it has the potential to be gamed and manipulated for a separate, unrelated benefit.  
The article is well-written, touching upon the various aspects that go into designing a research project, a prediction model no less. It also looks at the other aspects that influence big-data models, and how these influences can be reined in. Although it can be seen as a critique of the "new ways" of doing things, it raises important points about the boundaries between research and business, and the importance of regualarly reviewing models, both by creators as well as the community. It also makes good points about the ethics and good practices involved in the scientific community.  
Having said that, the paper doesn't do a great job at giving recommendations - it ends up suggesting generic recommendations, but does not give any specific directions in which the community should move when such issues arise. This can be one way this article can improve and live upto its promise.  