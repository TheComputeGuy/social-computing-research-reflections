### Data, privacy, and the greater good ([Link](https://erichorvitz.com/data_privacy_greater_good.pdf))

This policy article looks at large-scale social media data from the critical lens of potential threats to privacy while reaping the benefits that this data provides, looking at balancing innovation and regulation. This is a different approach than what we have seen in previous papers, specifically that this article looks at the policy perspectives rather than technology, science or ethics. It is also timely, given the burst of research that has been accelerated by social media data, both in the abstract fields of sociology, but also fields like public health and statistics, where poor data anonymization and handling PII can have real-world harms.  
The article discusses several key topics, one of which is how ML can facilitate leaps across contexts - using non-medical data to make inferences about health conditions (like social media data can help do), mentioning the difficulties that ML can pose when dealing with healthcare privacy. Given that most of social media data used for such inferences is public, there's a grave privacy danger that anyone with an internet connection and a computer can generate and use inferences about you from your digital trace. This also harks back to our previous study and discussion on predictions, where we studied the paper ["Private traits and attributes are predictable from digital records of human behavior"](https://www.pnas.org/doi/pdf/10.1073/pnas.1218772110), which analyzed digital trail and inferences.  
Additionally, the paper also makes mentions towards another hot topic today - gaps in legal frameworks when it comes to dealing with the tremendous rate of growth in technology, especially with regards to security, privacy and ML. The fact that there's no regulation on inferences and how these can be used is a dangerous time to live in, especially given the poor quality of inferences made. A company might take a certain decision about you based on a false inference, and it might still not fall afoul of laws in place today. All-in-all, multi-modal inference can lead to discrimination, and still be legal. The paper, with multiple examples, demonstrates, how differential privacy and deanonymization is difficult in practice, given how much data is available publicly to link multiple datasets together.  
The article also touches upon the various steps being taken worldwide, towards regulating data collection, privacy and inference, while also mentioning the practical difficulties of putting such bills into laws - be it political, technical, or otherwise. An interesting angle that the paper could use and didn't was looking at regulating data collection itself - it often mentions that the legal framework emphsizes data inference rather than collection, and that could be a new area of focus going forward.  

---

### Beyond the Belmont Principles: Ethical Challenges, Practices, and Beliefs in the Online Data Research Community ([Link](https://terpconnect.umd.edu/~kshilton/pdf/VitaketalCSCWpreprint.pdf))

In this paper, Vitak et al study the prevalent practices and ethics of collecting and analyzing online data among researchers, using a survey of social computing researchers. Using qualitative and quantitative methods, the authors documents beliefs and practices on which researchers converge, as well as the areas of disagreement. This is another novel lens of looking at this problem - understanding what the researchers themselves are thinking and considering when they collect and make use of potentially sensitive data for research. It is also an important problem to bring to the attention of the academic community and to start a meaningful discourse over what is ethical and acceptable in social computing research with large-scale data.  
This paper also closely tracks back to our discussion on ethics in handling data and inferences, that we had in our previous class on ethical challenges in social computing research, the aspect of consent, consent at scale, research oversight and access to data, multi-modal inference, even tangential talking points like default privacy settings and how they affect privacy and consent, who can be characterized as a participant and the more abstract discussion point about how we can design more private, secure and positive social systems going forward.  
One shortcoming of this research was that the research participants were heavily skewed towards the more regulated and informed western world, especially US and Europe. This ends up masking the issues and gaps seen in research conducted in countries with less oversight. It is in such research that there'll be concerns regarding privacy and ethics of data collection and inference. More often than not, such research does not go beyond the editorial stage of major publications, and hence, the paper ends up focusing on the more regulated and ethically sound western academic world. To counter this, the paper could have looked at a mix of different large and small publications rather than focusing on just the top publications, given that online data is accessible to anyone, rather than just a select set of researchers with access to specific dataset and funds.  
The study design, specifically the survey design process, was one thing that the paper did well - designing surveys is not easy, especially when you do not want to hint the participants towards a biased answer. The authors designed the survey based on interviews with scholars across different related fields, who were faculty or researchers. Thus, the authors were able to design an effective survey that was focused on the right issues and gleaned the right information. Overall, the methodologies used by the authors were very effective, and gave a good result spread to work with.  
There were some interesting snippets in the paper from the survey - regarding sharing of code and data, TOS violations v/s API use, researchers' view of the consent problem, etc. The paper also highlights how a lot of emphasis is about data inference, but not able not collecting data when not required in the first place, similar to the previous article. The shortcomings of IRBs, especially in technology research, can also be inferred from the survey responses. All of these have the potential to spawn off a lot of new areas of improvement as well as research into building effective community standards for conducting research on large-scale, potentially sensitive data.